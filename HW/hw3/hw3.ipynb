{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOHRLs9LRPI"
      },
      "source": [
        "# HW3: Profiling and Hyperparameter tuning\n",
        "\n",
        "You will gain experience with two practical tools in this assignment.\n",
        "\n",
        "* You will use the [TensorFlow Profiler](https://www.tensorflow.org/guide/profiler) to analyze an input pipeline for a slow running program, then add a few lines of code to improve performance. The profiler is built-in to [TensorBoard](https://www.tensorflow.org/tensorboard), a popular visualization tool (parts of this work with both TensorFlow and PyTorch).\n",
        "\n",
        "* You will use [Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) to tune the hyperparameters for a text classifier. Keras Tuner is an open-source hyperparamter tuning package that works with TensorFlow, scikit-learn, and other frameworks.\n",
        "\n",
        "Of course, you are welcome to use code from the website and book for this assignment. Please cite your sources (when in doubt, it never hurts to include a note re: the resources you used).\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "To submit this assignment, please upload a zip file to CourseWorks. Your zip file should include:\n",
        "* This notebook (saved, with output)\n",
        "* Screenshots of the TF Profiler as described below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d7y_mU0Dx8L"
      },
      "source": [
        "# Part 1: Profile a slow text classifier using the TensorFlow Profiler\n",
        "\n",
        "The most common peformance bottleneck in a DL program is the input pipeline. In a nutshell, modern GPUs are so fast they often sit idle while waiting for data to be loaded off disk, and/or preprocessed by the CPU (informally, this is called \"GPU starvation\").\n",
        "\n",
        "In this part of the assignment, you will improve the runtime performance of a text classifier (implemented below). The author of the text classifier has forgotten to optimize the input pipeline, causing it to run +/- **5x slower**, depending on your GPU.\n",
        "\n",
        "You will practice using the [TensorFlow Profiler](https://www.tensorflow.org/guide/profiler) to identify a performance problem, and [tf.data](https://www.tensorflow.org/guide/data_performance) to fix it. \n",
        "\n",
        "**Reading**\n",
        "\n",
        "You can learn about the [TF Profiler](https://www.tensorflow.org/guide/profiler) and [tf.data](https://www.tensorflow.org/guide/data_performance) with these links. Also see the [example code](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) for the profiler.\n",
        "\n",
        "For this assignment, you should use [caching](https://www.tensorflow.org/guide/data_performance#caching) and [prefetching](https://www.tensorflow.org/guide/data_performance#prefetching) to improve the input pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Vxp1bHf6aC"
      },
      "source": [
        "## Starter code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7slO8ZiyeAqb"
      },
      "source": [
        "The following code trains a simple text classifier on a [dataset](https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz) of programming questions extracted from Stack Overflow. This code is written for you. Each question (\"How do I sort a dictionary by value?\") is labeled with exactly one tag (`Python`, `CSharp`, `JavaScript`, or `Java`). The code below will train a model to predict the tag for a question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzqQe0SOhMpk"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "The goal of this section is for you to learn the mechanics of profiling code, so you can take advantage of it in your future work. Optimizing the input pipeline itself is not meant to be difficult (you can browse through existing tutorials on [tensorflow.org/tutorials](https://www.tensorflow.org/tutorials) to see how). I mainly want you to be aware of issues like these (so you can catch them in your own code down the road).\n",
        "\n",
        "After reading and and running this code to train a text classifier, you will:\n",
        "\n",
        "1. Modify the code below by adding a few lines to profile it with the TF Profiler\n",
        "1. Re-run the code, open the TF Profiler, and navigate to the section showing the input pipeline. Take a screenshot of the Profiler that shows the program is input-bound (and include the screenshot with your submission). \n",
        "1. Modify the code below by writing a few lines to optimize the data input pipeline, using [caching](https://www.tensorflow.org/guide/data_performance#caching) and [prefetching](https://www.tensorflow.org/guide/data_performance#prefetching).\n",
        "1. Re-run the code, and take a screenshot of the TF Profiler showing the input pipeline is faster (and include that screenshot with your submission).\n",
        "\n",
        "That's all you need to do for full credit on this section. If you're interested in diving deeper, both the profiler and tf.data are pretty extensive - you can continue reading on your own, and see what you can do to make the input pipeline even faster (there's a new feature called [snapshot](https://www.tensorflow.org/api_docs/python/tf/data/experimental/snapshot?version=nightly) you could try, for example, that was released a couple weeks ago).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "palxXVzSW9Yp"
      },
      "source": [
        "# Install the TF Profiler\n",
        "!pip install -U tensorboard_plugin_profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4ktq77ZT-hv"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "# If you are not running this notebook in https://colab.research.google.com/,\n",
        "# you will need to install TensorBoard on your machine.\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2b65xuLGQ3G"
      },
      "source": [
        "import collections\n",
        "import datetime\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYF4F61hhfpD"
      },
      "source": [
        "### Download and explore the dataset\n",
        "\n",
        "Download the dataset, and explore the directory structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ5n5K-OLs6-"
      },
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EELLY8ZMJ-fP"
      },
      "source": [
        "dataset = utils.get_file(\n",
        "    'stack_overflow_16k.tar.gz',\n",
        "    data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go8XfLixLr8P"
      },
      "source": [
        "dataset_dir = pathlib.Path(dataset).parent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-FW4TPlKVQM"
      },
      "source": [
        "list(dataset_dir.iterdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk1TiataMgLQ"
      },
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knyArCfehlng"
      },
      "source": [
        "The `train/csharp`, `train/java`, `train/python` and `train/javascript` directories contain many text files, each of which is a Stack Overflow question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jCaVxn-KWe0"
      },
      "source": [
        "sample_file = train_dir/'python/0.txt'\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PccgD9Yjhs4l"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Create an input pipeline to koad the data off disk and prepare it into a format suitable for training using [text_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory). This utility creates a labeled `tf.data.Dataset` from a directory structure as follows.\n",
        "\n",
        "```\n",
        "train/\n",
        "...csharp/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...java/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...javascript/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...python/\n",
        "......1.txt\n",
        "......2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6h-UHxTiGU6"
      },
      "source": [
        "This dataset has already been divided into train and test, but it lacks a validation set. Create a validation set using an 80:20 split of the training data by using the `validation_split` argument below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l52ZKbmDL0mP"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 0\n",
        "\n",
        "raw_train_ds = preprocessing.text_dataset_from_directory(\n",
        "  train_dir,\n",
        "  batch_size=batch_size,\n",
        "  validation_split=0.2,\n",
        "  subset='training',\n",
        "  seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0GrKRcniNqa"
      },
      "source": [
        "There are 8,000 examples in the training folder, of which you will use 80% (or 6,400) for training. As you will see in a moment, you can train a model by passing a `tf.data.Dataset` directly to `model.fit`. First, iterate over the dataset and print out a few examples, to get a feel for the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNlcYw7HKaBp"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Question: \", text_batch.numpy()[i])\n",
        "    print(\"Label:\", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo9qWA9CicUI"
      },
      "source": [
        "Note: To increase the difficulty of the classification problem, the dataset author replaced occurrences of the words *Python*, *CSharp*, *JavaScript*, or *Java* in the programming question with the word *blank*. This is to prevent the classifier from just memorizing a few words, and to make it more interesting to experiment with a few techniques.\n",
        "\n",
        "The labels are `0`, `1`, `2` or `3`. To see which of these correspond to which string label, you can check the `class_names` property on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lcCvSWuKbNd"
      },
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnvvjx_OikbV"
      },
      "source": [
        "Next, you will create a validation and test dataset. You will use the remaining 1,600 reviews from the training set for validation.\n",
        "\n",
        "Note:  When using the `validation_split` and `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation and training splits have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpAXywtwKciA"
      },
      "source": [
        "raw_val_ds = preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7_JXOKXioq0"
      },
      "source": [
        "Create a test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZDXd75tKfNA"
      },
      "source": [
        "test_dir = dataset_dir / 'test'\n",
        "raw_test_ds = preprocessing.text_dataset_from_directory(\n",
        "    test_dir, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAZEyrp4isUK"
      },
      "source": [
        "### Prepare the dataset for training\n",
        "\n",
        "Next, you will standardize, tokenize, and vectorize the data using the `preprocessing.TextVectorization` layer.\n",
        "\n",
        "* Standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset.\n",
        "\n",
        "* Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words by splitting on whitespace).\n",
        "\n",
        "* Vectorization refers to converting tokens into numbers so they can be fed into a neural network.\n",
        "\n",
        "All of these tasks can be accomplished with this layer. You can learn more about each of these in the [API doc](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n",
        "\n",
        "* The default standardization converts text to lowercase and removes punctuation.\n",
        "\n",
        "* The default tokenizer splits on whitespace.\n",
        "\n",
        "* The default vectorization mode is `int`. This outputs integer indices (one per token). This mode can be used to build models that take word order into account. Here, you will use `binary` to build a bag-of-word model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpz_ZKK0KgxO"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L62kDD8ui4ND"
      },
      "source": [
        "Next, you will call `adapt` using the training set. This will cause the `TextVectorization` layer to learn the vocabulary which it will use to convert sentences to a bag-of-words dataset.\n",
        "\n",
        "Note: it's important to only use your training data when calling adapt (using the validation or test set would leak information, of course)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F6U_wL2Kkal"
      },
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda text, labels: text)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L19L51fgjIoz"
      },
      "source": [
        "Now that the layer is ready, take a look at how it preprocesses text by calling it on a batch of data from the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubLkFQX5KmFm"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ4lj75SKnoH"
      },
      "source": [
        "# Retrieve a batch of 32 reviews and labels from the training set\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_question, first_label = text_batch[0], label_batch[0]\n",
        "\n",
        "print(\"Question\", first_question)\n",
        "print(\"Label\", first_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8dXFFh4jSOk"
      },
      "source": [
        "This is the result of applying the layer. The one-hot indicies correspond to the tokens present in the programming question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NALUvEO0Ko1_"
      },
      "source": [
        "one_hot_bow = vectorize_text(first_question, first_label)[0]\n",
        "print(\"Bag of words (one-hot encoded):\", one_hot_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30rQo3z5jXsf"
      },
      "source": [
        "The layer contains a vocabulary list that you can use to recover the original sentence (if using `int` mode), or to see the words selected by the one-hot encoding (if using `binary` mode). Of course, with binary mode word order is lost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo-vmRmLNMK3"
      },
      "source": [
        "vocab = vectorize_layer.get_vocabulary()\n",
        "words = tf.gather(vocab, tf.where(one_hot_bow[0]))\n",
        "print(\"Bag of words (tokens)\", words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouNj7jy5joq1"
      },
      "source": [
        "You are nearly ready to train your model. As a final preprocessing step, you will apply the `TextVectorization` layers you created earlier to the train, validation, and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jdp3rCuKqpa"
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8ViTRXFjtbM"
      },
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "The author has forgotten to implement this section. You will do so later after profiling the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpXKWpN0MH0j"
      },
      "source": [
        "##### \n",
        "# TODO: your code here\n",
        "# After profiling the code (and taking a screenshot of the slow input pipeline)\n",
        "# add code here to improve the performance\n",
        "#####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2oqqjg8j-Ac"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rje85KsKvQ_"
      },
      "source": [
        "model = tf.keras.Sequential([layers.Dense(4)])\n",
        "\n",
        "model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#####\n",
        "# TODO: your code here\n",
        "# Update and expand the code below to profile your program\n",
        "#####\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOX1fvM_gWpl"
      },
      "source": [
        "Notice this is a fairly small model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX5V4XtoKx72"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41a0ubO0kE2I"
      },
      "source": [
        "Evaluate the model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rMUWG5vKyGN"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbu6B7tSkG0o"
      },
      "source": [
        "### Export the model\n",
        "\n",
        "In the code above, you applied the `TextVectorization` layer to the dataset. If you want to make your model capable of processing raw strings (to simplify deploying it), you can include the `TextVectorization` layer inside your model. To do so, create a new model as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFghZbCDK1jA"
      },
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [vectorize_layer, model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lImjXBvZkS-8"
      },
      "source": [
        "Now your model can take raw strings as input and predict a score for each label using `model.predict`. Define a function to find the label with the maximum score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z-0f8RoK21G"
      },
      "source": [
        "def get_string_labels(predicted_scores_batch):\n",
        "  predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
        "  predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
        "  return predicted_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHyhq2aAkU5w"
      },
      "source": [
        "### Run inference on new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf8iX-Z1K4BG"
      },
      "source": [
        "inputs = [\n",
        "    \"how do I extract keys from a dict into a list?\",  # python\n",
        "    \"debug public static void main(string[] args) {...}\",  # java\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbtVHzp1kWLd"
      },
      "source": [
        "After reading and running the code above:\n",
        "1. Add code to profile it using the TF Profiler (search for \"# TODO: your code here\")\n",
        "1. Take a screenshot of the profiler showing a slow input pipeline (and include the screenshot with your submission)\n",
        "1. Add code to improve the performance of the input pipeline (search for \"# TODO: your code here\")\n",
        "1. Re-run the code, then take a screenshot of the profiler showing that your input pipeline is faster (and include the screenshot with your submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNoC_GjB50D_"
      },
      "source": [
        "# Part 2: Hyperparameter tuning\n",
        "\n",
        "There are many hyperparamters you can explore for your models (number and width of layers, various activations functions, weight initialization strategies, optimizers, etc). \n",
        "\n",
        "One way to tune these for a model you're developing is to use [Keras Tuner](https://keras-team.github.io/keras-tuner/), an open-source hyperparameter tuner that works with TensorFlow, sklearn, and other frameworks.\n",
        "\n",
        "In this part of the assignment, you will use Keras Tuner to improve a text classification model published on https://tensorflow.org. In the previous part of the assignment, your example code was based on [this tutorial](https://www.tensorflow.org/tutorials/load_data/text) (recently published, just a week ago).\n",
        "\n",
        "A developer recently updated this tutorial, but they did not tune the `int` text classification model from Part 1:\n",
        "\n",
        "```\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "    layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
        "    layers.GlobalMaxPooling1D(),\n",
        "    layers.Dense(num_labels)\n",
        "])\n",
        "```\n",
        "\n",
        "Above is the `int` model from part one of that tutorial. Although it has 660,868 parameters (compared to just 40,004 for the `binary` model above it in the tutorial!), the validation accuracy of the `int` model is lower. \n",
        "\n",
        "**Instructions**\n",
        "\n",
        "Use Keras Tuner to improve the `int` model. There are several strategies you can use:\n",
        "\n",
        "* You can explore alternative sizes for the Embedding and Conv1D layers (perhaps more or fewer neurons are helpful?) \n",
        "* You can explore different arrangements and types of layers.\n",
        "* If you like, you can experiment with RNNs (you can find example code [here](https://www.tensorflow.org/tutorials/text/text_classification_rnn)).\n",
        "\n",
        "You can improve the model in one of several ways (or ideally, multiple). Your improved model can:\n",
        "* Have higher accuracy on the validation set\n",
        "* Be smaller (fewer paramaters)\n",
        "* Can be faster to train (wall clock time)\n",
        "\n",
        "Add code below to install Keras Tuner, and optimize this model. After you've finished running your experiments, add a text cell below that lists:\n",
        "* Your best model (inside ``` quotes ```, to make it easy to read)\n",
        "* The validation accuracy on the same dataset from that tutorial\n",
        "* Briefly describes the experiments you ran to improve it. \n",
        "\n",
        "**Recommended reading**\n",
        "\n",
        "Keras Tuner\n",
        "* [Blog post](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)\n",
        "* [Tutorial](https://www.tensorflow.org/tutorials/keras/keras_tuner)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZuvIvrO9ISr"
      },
      "source": [
        "#####\n",
        "# TODO: your code here\n",
        "# Install Keras Tuner, and use it to run experiments\n",
        "# to improve the `int` text classification model \n",
        "# discussed above.\n",
        "# Include your complete code with your submission in this notebook\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0LNrHdPZdWR"
      },
      "source": [
        "TODO: your write-up here. In this text cell, briefly list:\n",
        "* Your best model (inside ``` quotes ```)\n",
        "* The validation accuracy on the same dataset from that tutorial\n",
        "* The experiments you ran to improve it.  "
      ]
    }
  ]
}